{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749596d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file salvato in: logs/20250708_153353/model_evaluation.log\n",
      "\n",
      "========================================\n",
      "Random Search + Training Stage 1: Healthy vs Infected (SVM)\n",
      "========================================\n",
      "Running RandomizedSearchCV for Stage 1 - SVM ...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achille/Desktop/ML/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=15.0min\n",
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=20.4min\n",
      "[CV] END classifier__C=100, classifier__gamma=auto, classifier__kernel=poly, selector__k=700; total time=25.9min\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "svm = False\n",
    "\n",
    "# === Create timestamped logging folder ===\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_dir = f\"logs/{timestamp}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# === Logging setup (robusto) ===\n",
    "log_file = os.path.join(log_dir, \"model_evaluation.log\")\n",
    "\n",
    "# Rimuove handler pre-esistenti (es. da Jupyter o da un run precedente)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Configura logging verso file\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Test immediato per confermare che il file venga scritto\n",
    "logging.info(\"Logger inizializzato con successo.\")\n",
    "\n",
    "def save_plot(fig, name):\n",
    "    fig_path = os.path.join(log_dir, name)\n",
    "    fig.savefig(fig_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "print(f\"Log file salvato in: {log_file}\")\n",
    "\n",
    "def recode_labels_for_first_classifier(labels):\n",
    "    return labels.apply(lambda x: \"healthy\" if x in [0, 1] else \"infected\")\n",
    "\n",
    "def filter_data_for_second_classifier(X, y):\n",
    "    mask = y.isin([2, 3, 4, 5, 6])\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"\\n--- Evaluation: {model_name} ---\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(matrix)\n",
    "    print(f\"Balanced Accuracy: {balanced_acc}\")\n",
    "    print(f\"F1 Macro: {f1_macro}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    logging.info(f\"\\n--- Evaluation: {model_name} ---\")\n",
    "    logging.info(f\"Classification Report:\\n{report}\")\n",
    "    logging.info(f\"Confusion Matrix:\\n{matrix}\")\n",
    "    logging.info(f\"Balanced Accuracy: {balanced_acc}\")\n",
    "    logging.info(f\"F1 Macro: {f1_macro}\")\n",
    "    logging.info(\"-\" * 80)\n",
    "\n",
    "    # figures\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    save_plot(plt.gcf(), f\"confusion_matrix_{model_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "    # Plot\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    save_plot(plt.gcf(), f\"precision_recall_curve_{model_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # scatter plot of the fist two features\n",
    "    if X_test.shape[1] >= 2:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_pred, cmap='viridis', alpha=0.5)\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.title(f'Scatter Plot of First Two Features - {model_name}')\n",
    "        save_plot(plt.gcf(), f'scatter_plot_{model_name}.png')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Pipeline 1: XGBoost\n",
    "def create_xgb_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', XGBClassifier(eval_metric='mlogloss', random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 6, 10],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 2: Random Forest\n",
    "def create_rf_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 3: SVM\n",
    "def create_svm_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', SVC(random_state=42, class_weight='balanced', kernel='linear', probability=True))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__C': [100],\n",
    "        'classifier__kernel': ['poly'],\n",
    "        'classifier__gamma': ['auto']  # Only relevant for rbf and poly kernels\n",
    "    }\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 4: Logistic Regression\n",
    "def create_lr_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 5: KNN\n",
    "def create_knn_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__p': [1, 2]  # 1: manhattan, 2: euclidean\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "def run_random_search(pipeline, param_dist, X_train, y_train, model_name=\"Model\", n_iter=10):\n",
    "    print(f\"Running RandomizedSearchCV for {model_name} ...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        scoring='f1_macro',\n",
    "        n_iter=n_iter,           # Number of random combinations to try\n",
    "        n_jobs=2,                # Parallel jobs (adjust based on RAM)\n",
    "        cv=3,                    # Cross-validation folds\n",
    "        verbose=2,\n",
    "        random_state=42,         # For reproducibility\n",
    "        refit=True\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "    print(f\"Best F1 Macro Score for {model_name}: {random_search.best_score_:.4f}\")\n",
    "    logging.info(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "    logging.info(f\"Best F1 Macro Score for {model_name}: {random_search.best_score_:.4f}\")\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"../roi_features_train.csv\")\n",
    "    X = df.drop(columns=[\"image_id\", \"score\", \"x1\", \"y1\", \"x2\", \"y2\", \"label\"])\n",
    "    y_original = df[\"label\"]\n",
    "\n",
    "    y_stage1 = recode_labels_for_first_classifier(y_original)\n",
    "    le_stage1 = LabelEncoder()\n",
    "    y_stage1_encoded = le_stage1.fit_transform(y_stage1)\n",
    "\n",
    "    X_train_stage1, X_test_stage1, y_train_stage1, y_test_stage1 = train_test_split(\n",
    "        X, y_stage1_encoded, test_size=0.2, stratify=y_stage1_encoded, random_state=42\n",
    "    )\n",
    "\n",
    "    pipelines = [\n",
    "        #(\"XGBoost\", create_xgb_pipeline),\n",
    "        #(\"Random Forest\", create_rf_pipeline),\n",
    "        (\"SVM\", create_svm_pipeline),\n",
    "        #(\"Logistic Regression\", create_lr_pipeline),\n",
    "        #(\"KNN\", create_knn_pipeline)\n",
    "    ]\n",
    "\n",
    "    \n",
    "    name1 = 'SVM'\n",
    "    pipeline_stage1, param_dist_stage1 = create_svm_pipeline(num_class=2)\n",
    "    logging.info(f\"\\n{'='*40}\\nRandom Search + Training Stage 1: Healthy vs Infected ({name1})\\n{'='*40}\")\n",
    "    print(f\"\\n{'='*40}\\nRandom Search + Training Stage 1: Healthy vs Infected ({name1})\\n{'='*40}\")\n",
    "\n",
    "    # Run GridSearchCV\n",
    "    best_pipeline_stage1 = run_random_search(pipeline_stage1, param_dist_stage1, X_train_stage1, y_train_stage1, model_name=f\"Stage 1 - {name1}\")\n",
    "    \n",
    "    evaluate_model(best_pipeline_stage1, X_test_stage1, y_test_stage1, model_name=f\"Stage 1 - {name1}\")\n",
    "\n",
    "    y_pred_stage1 = best_pipeline_stage1.predict(X_test_stage1)\n",
    "    infected_indices = np.where(y_pred_stage1 == 1)[0]\n",
    "\n",
    "    if len(infected_indices) == 0:\n",
    "        logging.warning(f\"No infected cells predicted by Stage 1 ({name1}) on the test set.\")\n",
    "        print(f\"No infected cells predicted by Stage 1 ({name1}) on the test set.\")\n",
    "        #continue\n",
    "\n",
    "    X_test_stage2 = X_test_stage1.iloc[infected_indices]\n",
    "    allowed_labels = [2, 3, 4, 5, 6]\n",
    "    mask_test_stage2 = y_original.iloc[X_test_stage2.index].isin(allowed_labels)\n",
    "    X_test_stage2 = X_test_stage2[mask_test_stage2]\n",
    "    y_test_stage2 = y_original.iloc[X_test_stage2.index]\n",
    "\n",
    "    X_train_stage2, y_train_stage2 = filter_data_for_second_classifier(\n",
    "        X_train_stage1, y_original.iloc[X_train_stage1.index]\n",
    "    )\n",
    "\n",
    "    le_stage2 = LabelEncoder()\n",
    "    y_train_stage2_encoded = le_stage2.fit_transform(y_train_stage2)\n",
    "    try:\n",
    "        y_test_stage2_encoded = le_stage2.transform(y_test_stage2)\n",
    "    except ValueError:\n",
    "        unseen = set(y_test_stage2) - set(le_stage2.classes_)\n",
    "        logging.warning(f\"Skipping Stage 2 ({name1}) due to unseen labels in test: {unseen}\")\n",
    "        print(f\"Skipping Stage 2 ({name1}) due to unseen labels in test: {unseen}\")\n",
    "        #continue\n",
    "\n",
    "    num_classes_stage2 = len(le_stage2.classes_)\n",
    "    logging.info(f\"Number of classes in Stage 2: {num_classes_stage2}\")\n",
    "    print(f\"Number of classes in Stage 2: {num_classes_stage2}\")\n",
    "\n",
    "    for name2, pipeline_func2 in pipelines:\n",
    "        logging.info(f\"\\n{'='*40}\\nRandom Search + Training Stage 2: Infected Subtype Classification ({name1} ➡ {name2})\\n{'='*40}\")\n",
    "        print(f\"\\n{'='*40}\\nRandom Search + Training Stage 2: Infected Subtype Classification ({name1} ➡ {name2})\\n{'='*40}\")\n",
    "        pipeline_stage2, param_dist_stage2 = pipeline_func2(num_class=num_classes_stage2)\n",
    "\n",
    "        # Run GridSearchCV\n",
    "        best_pipeline_stage2 = run_random_search(pipeline_stage2, param_dist_stage2, X_train_stage2, y_train_stage2_encoded, model_name=f\"Stage 2 - {name1} ➡ {name2}\")\n",
    "\n",
    "        evaluate_model(best_pipeline_stage2, X_test_stage2, y_test_stage2_encoded, model_name=f\"Stage 2 - {name1} ➡ {name2}\")\n",
    "\n",
    "    print(\"✅ Pipeline with Random Search completed successfully.\")\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        handler.flush()\n",
    "        handler.close()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
