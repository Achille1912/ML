{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749596d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file salvato in: logs/20250703_120727/model_evaluation.log\n",
      "\n",
      "\n",
      "========================================\n",
      "Grid Search + Training Stage 1: Healthy vs Infected (SVM)\n",
      "========================================\n",
      "Running GridSearchCV for Stage 1 - SVM ...\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END classifier__C=0.001, classifier__gamma=scale, classifier__kernel=linear, selector__k=700; total time=  56.0s\n",
      "[CV] END classifier__C=0.001, classifier__gamma=scale, classifier__kernel=linear, selector__k=700; total time=  56.8s\n",
      "[CV] END classifier__C=0.001, classifier__gamma=scale, classifier__kernel=linear, selector__k=700; total time=  51.8s\n",
      "[CV] END classifier__C=0.001, classifier__gamma=scale, classifier__kernel=rbf, selector__k=700; total time=  52.3s\n",
      "[CV] END classifier__C=0.001, classifier__gamma=scale, classifier__kernel=rbf, selector__k=700; total time=  50.0s\n",
      "[CV] END classifier__C=0.001, classifier__gamma=scale, classifier__kernel=rbf, selector__k=700; total time=  53.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achille/Desktop/ML/.venv/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Logging Setup\n",
    "log_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"logs/{log_time}/model_evaluation.log\"\n",
    "os.makedirs(os.path.dirname(log_filename), exist_ok=True)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO)\n",
    "print(f\"Log file salvato in: {log_filename}\\n\")\n",
    "\n",
    "def recode_labels_for_first_classifier(labels):\n",
    "    return labels.apply(lambda x: \"healthy\" if x in [0, 1] else \"infected\")\n",
    "\n",
    "def filter_data_for_second_classifier(X, y):\n",
    "    mask = y.isin([2, 3, 4, 5, 6])\n",
    "    return X[mask], y[mask]\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"\\n--- Evaluation: {model_name} ---\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(matrix)\n",
    "    print(f\"Balanced Accuracy: {balanced_acc}\")\n",
    "    print(f\"F1 Macro: {f1_macro}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    logging.info(f\"\\n--- Evaluation: {model_name} ---\")\n",
    "    logging.info(f\"Classification Report:\\n{report}\")\n",
    "    logging.info(f\"Confusion Matrix:\\n{matrix}\")\n",
    "    logging.info(f\"Balanced Accuracy: {balanced_acc}\")\n",
    "    logging.info(f\"F1 Macro: {f1_macro}\")\n",
    "    logging.info(\"-\" * 80)\n",
    "\n",
    "# Pipeline 1: XGBoost\n",
    "def create_xgb_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', XGBClassifier(eval_metric='mlogloss', random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 6, 10],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 2: Random Forest\n",
    "def create_rf_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 5, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 3: SVM\n",
    "def create_svm_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', SVC(random_state=42, class_weight='balanced', kernel='linear'))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'classifier__gamma': ['scale', 'auto']  # Only relevant for rbf and poly kernels\n",
    "    }\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 4: Logistic Regression\n",
    "def create_lr_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "# Pipeline 5: KNN\n",
    "def create_knn_pipeline(num_class):\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', BorderlineSMOTE(random_state=42)),\n",
    "        ('varthresh', VarianceThreshold(threshold=1e-5)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=100)),\n",
    "        ('lda', LinearDiscriminantAnalysis(solver='svd', n_components=num_class-1)),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'selector__k': [700],\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__p': [1, 2]  # 1: manhattan, 2: euclidean\n",
    "    }\n",
    "\n",
    "    return pipeline, param_dist\n",
    "\n",
    "def run_grid_search(pipeline, param_grid, X_train, y_train, model_name=\"Model\"):\n",
    "    print(f\"Running GridSearchCV for {model_name} ...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=2,\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        refit=True\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best F1 Macro Score for {model_name}: {grid_search.best_score_:.4f}\")\n",
    "    logging.info(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    logging.info(f\"Best F1 Macro Score for {model_name}: {grid_search.best_score_:.4f}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"../roi_features_train.csv\")\n",
    "    X = df.drop(columns=[\"image_id\", \"score\", \"x1\", \"y1\", \"x2\", \"y2\", \"label\"])\n",
    "    y_original = df[\"label\"]\n",
    "\n",
    "    y_stage1 = recode_labels_for_first_classifier(y_original)\n",
    "    le_stage1 = LabelEncoder()\n",
    "    y_stage1_encoded = le_stage1.fit_transform(y_stage1)\n",
    "\n",
    "    X_train_stage1, X_test_stage1, y_train_stage1, y_test_stage1 = train_test_split(\n",
    "        X, y_stage1_encoded, test_size=0.2, stratify=y_stage1_encoded, random_state=42\n",
    "    )\n",
    "\n",
    "    pipelines = [\n",
    "        #(\"XGBoost\", create_xgb_pipeline),\n",
    "        (\"Random Forest\", create_rf_pipeline),\n",
    "        (\"SVM\", create_svm_pipeline),\n",
    "        (\"Logistic Regression\", create_lr_pipeline),\n",
    "        (\"KNN\", create_knn_pipeline)\n",
    "    ]\n",
    "\n",
    "     # Use fixed classifier for Stage 1\n",
    "    name1 = \"SVM\"  # or \"SVM\", \"Random Forest\", etc.\n",
    "    pipeline_func1 = dict(pipelines)[name1]\n",
    "    print(f\"\\n{'='*40}\\nGrid Search + Training Stage 1: Healthy vs Infected ({name1})\\n{'='*40}\")\n",
    "    pipeline_stage1, param_dist_stage1 = pipeline_func1(num_class=2)\n",
    "\n",
    "    # Run GridSearchCV\n",
    "    best_pipeline_stage1 = run_grid_search(pipeline_stage1, param_dist_stage1, X_train_stage1, y_train_stage1, model_name=f\"Stage 1 - {name1}\")\n",
    "\n",
    "    evaluate_model(best_pipeline_stage1, X_test_stage1, y_test_stage1, model_name=f\"Stage 1 - {name1}\")\n",
    "\n",
    "    y_pred_stage1 = best_pipeline_stage1.predict(X_test_stage1)\n",
    "    infected_indices = np.where(y_pred_stage1 == 1)[0]\n",
    "\n",
    "    X_test_stage2 = X_test_stage1.iloc[infected_indices]\n",
    "    allowed_labels = [2, 3, 4, 5, 6]\n",
    "    mask_test_stage2 = y_original.iloc[X_test_stage2.index].isin(allowed_labels)\n",
    "    X_test_stage2 = X_test_stage2[mask_test_stage2]\n",
    "    y_test_stage2 = y_original.iloc[X_test_stage2.index]\n",
    "\n",
    "    X_train_stage2, y_train_stage2 = filter_data_for_second_classifier(\n",
    "        X_train_stage1, y_original.iloc[X_train_stage1.index]\n",
    "    )\n",
    "\n",
    "    le_stage2 = LabelEncoder()\n",
    "    y_train_stage2_encoded = le_stage2.fit_transform(y_train_stage2)\n",
    "    try:\n",
    "        y_test_stage2_encoded = le_stage2.transform(y_test_stage2)\n",
    "    except ValueError:\n",
    "        unseen = set(y_test_stage2) - set(le_stage2.classes_)\n",
    "        print(f\"Skipping Stage 2 ({name1}) due to unseen labels in test: {unseen}\")\n",
    "        \n",
    "\n",
    "    num_classes_stage2 = len(le_stage2.classes_)\n",
    "\n",
    "    for name2, pipeline_func2 in pipelines:\n",
    "        print(f\"\\n{'='*40}\\nGrid Search + Training Stage 2: Infected Subtype Classification ({name1} ➡ {name2})\\n{'='*40}\")\n",
    "        pipeline_stage2, param_dist_stage2 = pipeline_func2(num_class=num_classes_stage2)\n",
    "\n",
    "        # Run GridSearchCV\n",
    "        best_pipeline_stage2 = run_grid_search(pipeline_stage2, param_dist_stage2, X_train_stage2, y_train_stage2_encoded, model_name=f\"Stage 2 - {name1} ➡ {name2}\")\n",
    "\n",
    "        evaluate_model(best_pipeline_stage2, X_test_stage2, y_test_stage2_encoded, model_name=f\"Stage 2 - {name1} ➡ {name2}\")\n",
    "\n",
    "    print(\"✅ Pipeline with Grid Search completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
